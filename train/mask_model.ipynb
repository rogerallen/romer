{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Model\n",
    "-------\n",
    "\n",
    "Basing this on the fast.ai Keras Tiramisu implementation of the One Hundred Layers Tiramisu as described in Simon Jegou et al.'s paper [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/abs/1611.09326).\n",
    "\n",
    "This is heavily inspired by the code from the 2nd Fast.ai Deep Learning course here:\n",
    "https://github.com/fastai/courses/blob/master/deeplearning2/tiramisu-keras.ipynb\n",
    "\n",
    "I've updated it to Keras 2 and have rewritten it to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rallen/anaconda3/envs/dl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#import bcolz\n",
    "import glob\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, Activation, Dropout, concatenate, Conv2DTranspose, Reshape\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import threading\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_block_down(x,nb_layers,growth_rate,p,wd):\n",
    "    \"\"\"See Figure 2 from th paper. A dense_block has nb_layers of 'Layer' blocks.  This function \n",
    "    takes in an input x, and returns two output variants: a modified x and a 'added' variable.  \n",
    "    Note in Figure 1 where the down path has a post-DB concat to the x input and the up connection does not.  \n",
    "    x becomes a concatenation of x and the intermediate layer outputs: [x,a,a',a'',a''',...] and \n",
    "    'added' variable is concatenated layer of only the layer outputs [a,a',a'',a''',...] \n",
    "    So this allows this one bit of code to work for both down and up connections.  \n",
    "    \n",
    "    Each layer is a described in Table 1 of the paper.  Batch Norm, ReLU, 3x3 Conv, Dropout (p=0.2)\n",
    "    \n",
    "    This seems like something we don't need to be so tricky about.  It could \n",
    "    be eating more memory than is necessary to make both variables.  \n",
    "    But at the moment I don't know how to code keeping \n",
    "    an input to this function unmodified to \"go around\" this dense_block.\n",
    "    \"\"\"\n",
    "    for i in range(nb_layers):\n",
    "        a = BatchNormalization(axis=-1)(x)\n",
    "        a = Activation('relu')(a)\n",
    "        a = Conv2D(growth_rate, (3, 3),\n",
    "                   padding=\"same\", kernel_initializer=\"he_uniform\", \n",
    "                   kernel_regularizer=l2(wd))(a)\n",
    "        a = Dropout(p)(a) if p else a\n",
    "        x = concatenate([x, a])\n",
    "    return x\n",
    "\n",
    "def dense_block_up(x,nb_layers,growth_rate,p,wd):\n",
    "    \"\"\"FIXME\n",
    "    \"\"\"\n",
    "    for i in range(nb_layers):\n",
    "        a = BatchNormalization(axis=-1)(x)\n",
    "        a = Activation('relu')(a)\n",
    "        a = Conv2D(growth_rate, (3, 3),\n",
    "                   padding=\"same\", kernel_initializer=\"he_uniform\", \n",
    "                   kernel_regularizer=l2(wd))(a)\n",
    "        a = Dropout(p)(a) if p else a\n",
    "        x = concatenate([x, a])\n",
    "        if i == 0:\n",
    "            added = a\n",
    "        else:\n",
    "            added = concatenate([added,a]) \n",
    "    return added\n",
    "\n",
    "def transition_down(x, p, wd, do_maxpool):\n",
    "    \"\"\"See Table 1 for the Transition Down block.  Similar to the Layer \n",
    "    in the DenseBlock, but slightly different with 1x1 Conv and 2x2 MaxPooling.\n",
    "    Jeremy Howard's fast.ai code did not use MaxPooling, but rather just 2x strides,\n",
    "    so allow for control of this via do_maxpool.\"\"\"\n",
    "    nb_filter = x.get_shape().as_list()[-1]\n",
    "    conv_stride = 1 if do_maxpool else 2\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(nb_filter, (1, 1),\n",
    "               padding=\"same\", kernel_initializer=\"he_uniform\", \n",
    "               strides=(conv_stride, conv_stride),\n",
    "               kernel_regularizer=l2(wd))(x)\n",
    "    x = Dropout(p)(x) if p else x\n",
    "    x = MaxPooling2D()(x) if do_maxpool else x\n",
    "    return x\n",
    "\n",
    "def transition_up(added, wd=0):\n",
    "    \"\"\"See Table 1 for the transition up block. A 3x3 Transposed Conv with stride=2.\"\"\"\n",
    "    _,r,c,ch = added.get_shape().as_list()\n",
    "    x = Conv2DTranspose(ch, (3, 3), \n",
    "                        strides=(2, 2), \n",
    "                        padding=\"same\", \n",
    "                        kernel_initializer=\"he_uniform\", \n",
    "                        kernel_regularizer=l2(wd))(added)\n",
    "    return x\n",
    "\n",
    "def create_tiramisu_model(nb_classes, img_input, nb_layers_per_block, \n",
    "                          initial_filter=48, bottleneck_filter=15, growth_rate=16, \n",
    "                          do_td_maxpool=True, p=0.2, wd=1e-4):\n",
    "    \"\"\"create_tiramisu_model:\n",
    "    inputs:\n",
    "      nb_classes: number of output classes to classify\n",
    "      img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
    "          that should match your K.image_data_format() == 'channels_first' setting.\n",
    "      nb_layers_per_block: \n",
    "          items in list are number of layers in each dense block (not including bottleneck)\n",
    "          e.g. [4,5,7,10,12] to match the paper.  Note that the last entry in the list \n",
    "          doesn't produce skips and the up path won't mirror it.\n",
    "      initial_filter: number of filters in initial 3x3 Conv (48 per paper)\n",
    "      bottleneck_filter: number of filters in bottleneck stage (15 per paper)\n",
    "      growth_rate: number of filters to add per dense block (12 or 16 per paper)\n",
    "      do_td_maxpool: Jeremy Howard says \n",
    "      p: dropout rate or None for no Dropout() (0.2 per paper)\n",
    "      wd: weight decay (1e-4 per paper)\n",
    "    output:\n",
    "      Tiramisu Keras Model\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(nb_layers_per_block) == list\n",
    "    \n",
    "    # initial 3x3 Convolution\n",
    "    x = Conv2D(initial_filter, (3, 3), padding=\"same\", \n",
    "               kernel_initializer=\"he_uniform\", \n",
    "               kernel_regularizer=l2(wd))(img_input)\n",
    " \n",
    "    # Down Path: DenseBlocks + TransitionDowns\n",
    "    skips = []\n",
    "    for nb_layers in nb_layers_per_block:\n",
    "        x = dense_block_down(x, nb_layers, growth_rate, p, wd)\n",
    "        skips.append(x)\n",
    "        x = transition_down(x, p, wd, do_td_maxpool)\n",
    "    \n",
    "    # Bottleneck\n",
    "    added = dense_block_up(x, nb_layers, growth_rate, p, wd)\n",
    "    \n",
    "    # Up Path: TransitionUp + DenseBlocks\n",
    "    skips = list(reversed(skips))\n",
    "    nb_layers_per_block = list(reversed(nb_layers_per_block))\n",
    "    for i,nb_layers in enumerate(nb_layers_per_block):\n",
    "        x = transition_up(added, wd)\n",
    "        x = concatenate([x,skips[i]])\n",
    "        added = dense_block_up(x, nb_layers, growth_rate, p, wd)\n",
    "    \n",
    "    # final 1x1 Convolution & Softmax\n",
    "    x = Conv2D(nb_classes, (1, 1), padding=\"same\", \n",
    "               kernel_initializer=\"he_uniform\", \n",
    "               kernel_regularizer=l2(wd))(x)\n",
    "    x = Reshape((-1, nb_classes))(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return x\n",
    "\n",
    "class image_generator(object):\n",
    "    def __init__(self, X, Y, batch_size, channels):\n",
    "        self.X  = X\n",
    "        self.Y  = Y\n",
    "        self.bs = batch_size\n",
    "        self.i  = 0\n",
    "    def __next__(self):\n",
    "        xs = self.X[self.i:self.i+self.bs]\n",
    "        ys = self.Y[self.i:self.i+self.bs]\n",
    "        ys = ys.reshape(len(ys),-1,channels) # convert 32,64,64,1 -> 32,4096,1\n",
    "        self.i = (self.i + self.bs) % self.X.shape[0]\n",
    "        return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the training & validation images\n",
    "-------\n",
    "\n",
    "These are created via the mask_data_prep notebook\n",
    "\n",
    " - [ ] FIXME add testing of the test image set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember to adjust 255 -> 1 on mask to convert it from a mask to a label\n",
    "# 0=background, 1=inside staff\n",
    "train_path = 'data/train'\n",
    "train_score_image_names = glob.glob(train_path+'/diced_score*png')\n",
    "train_mask_image_names = glob.glob(train_path+'/diced_mask*png')\n",
    "train_score_image_names.sort()\n",
    "train_mask_image_names.sort()\n",
    "train_score_images = np.stack([np.array(Image.open(fn)) for fn in train_score_image_names])\n",
    "train_mask_images = np.stack([np.array(Image.open(fn))//255 for fn in train_mask_image_names])\n",
    "train_score_images.shape = train_score_images.shape + (1,)\n",
    "train_mask_images.shape = train_mask_images.shape + (1,)\n",
    "assert(train_score_images.shape == train_mask_images.shape)\n",
    "num_train_images,rows,cols,channels = train_score_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_path = 'data/valid'\n",
    "valid_score_image_names = glob.glob(valid_path+'/diced_score*png')\n",
    "valid_mask_image_names = glob.glob(valid_path+'/diced_mask*png')\n",
    "valid_score_image_names.sort()\n",
    "valid_mask_image_names.sort()\n",
    "valid_score_images = np.stack([np.array(Image.open(fn)) for fn in valid_score_image_names])\n",
    "# adjust 255 -> 1 on mask\n",
    "valid_mask_images = np.stack([np.array(Image.open(fn))//255 for fn in valid_mask_image_names])\n",
    "valid_score_images.shape = valid_score_images.shape + (1,)\n",
    "valid_mask_images.shape = valid_mask_images.shape + (1,)\n",
    "assert(valid_score_images.shape == valid_mask_images.shape)\n",
    "num_valid_images,_,_,_ = valid_score_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1780, 592, 64, 64, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_images, num_valid_images, rows, cols, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc4bcddbc88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEIqfJEIqfBFIvR//COW9/OG/LcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4bce86908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NNN=501\n",
    "plt.imshow(train_score_images[NNN].reshape(64,64),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc45eff7eb8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc45f087080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_mask_images[NNN].reshape(64,64),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the Model & Do some Training\n",
    "-----\n",
    "\n",
    "Just a few epochs to see how things go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (rows,cols,channels)\n",
    "batch_size  = 32\n",
    "num_epochs  = 20\n",
    "train_rate  = 1e-4\n",
    "num_labels  = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = image_generator(train_score_images, train_mask_images, batch_size, channels)\n",
    "valid_generator = image_generator(valid_score_images, valid_mask_images, batch_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_input = Input(shape=input_shape)\n",
    "x = create_tiramisu_model(num_labels, image_input, \n",
    "                          nb_layers_per_block=[4,5,7,10,12],\n",
    "                          initial_filter=48, bottleneck_filter=15, growth_rate=16,\n",
    "                          do_td_maxpool=False,\n",
    "                          p=0.2, wd=1e-4)\n",
    "model = Model(image_input, x)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(train_rate), \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "27s - loss: 10.9791 - acc: 0.3546 - val_loss: 11.1092 - val_acc: 0.3333\n",
      "Epoch 2/20\n",
      "21s - loss: 10.8090 - acc: 0.3604 - val_loss: 10.8976 - val_acc: 0.3557\n",
      "Epoch 3/20\n",
      "21s - loss: 4.6621 - acc: 0.7248 - val_loss: 1.7459 - val_acc: 0.9168\n",
      "Epoch 4/20\n",
      "21s - loss: 1.3639 - acc: 0.9400 - val_loss: 1.0072 - val_acc: 0.9670\n",
      "Epoch 5/20\n",
      "21s - loss: 1.0288 - acc: 0.9647 - val_loss: 0.9839 - val_acc: 0.9678\n",
      "Epoch 6/20\n",
      "21s - loss: 0.8998 - acc: 0.9738 - val_loss: 1.1267 - val_acc: 0.9592\n",
      "Epoch 7/20\n",
      "22s - loss: 0.8331 - acc: 0.9785 - val_loss: 0.8773 - val_acc: 0.9754\n",
      "Epoch 8/20\n",
      "21s - loss: 0.7574 - acc: 0.9834 - val_loss: 0.8637 - val_acc: 0.9764\n",
      "Epoch 9/20\n",
      "21s - loss: 0.7468 - acc: 0.9836 - val_loss: 0.7920 - val_acc: 0.9804\n",
      "Epoch 10/20\n",
      "21s - loss: 0.6923 - acc: 0.9871 - val_loss: 0.6926 - val_acc: 0.9868\n",
      "Epoch 11/20\n",
      "21s - loss: 0.6632 - acc: 0.9888 - val_loss: 0.6956 - val_acc: 0.9865\n",
      "Epoch 12/20\n",
      "21s - loss: 0.6291 - acc: 0.9908 - val_loss: 0.7147 - val_acc: 0.9841\n",
      "Epoch 13/20\n",
      "21s - loss: 0.6066 - acc: 0.9917 - val_loss: 0.6175 - val_acc: 0.9907\n",
      "Epoch 14/20\n",
      "22s - loss: 0.6092 - acc: 0.9914 - val_loss: 0.5900 - val_acc: 0.9926\n",
      "Epoch 15/20\n",
      "21s - loss: 0.5871 - acc: 0.9923 - val_loss: 0.5874 - val_acc: 0.9917\n",
      "Epoch 16/20\n",
      "21s - loss: 0.5659 - acc: 0.9934 - val_loss: 0.5931 - val_acc: 0.9907\n",
      "Epoch 17/20\n",
      "21s - loss: 0.5484 - acc: 0.9942 - val_loss: 0.5464 - val_acc: 0.9940\n",
      "Epoch 18/20\n",
      "21s - loss: 0.5338 - acc: 0.9946 - val_loss: 0.5293 - val_acc: 0.9947\n",
      "Epoch 19/20\n",
      "21s - loss: 0.5278 - acc: 0.9947 - val_loss: 0.5509 - val_acc: 0.9926\n",
      "Epoch 20/20\n",
      "21s - loss: 0.5069 - acc: 0.9956 - val_loss: 0.5032 - val_acc: 0.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc75f4975f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, \n",
    "                    num_train_images//batch_size, num_epochs, \n",
    "                    verbose=2,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=num_valid_images//batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, the above output pretty closely matches the below output so the code changes seem good.\n",
    "\n",
    "Continuing to adjust code... \n",
    "Wow, changing to separate dense_block_up/down makes big difference to runtime.\n",
    "now 22s/epoch & was 32s/epoch!  \n",
    "\n",
    "Comparison to original working code:\n",
    "```\n",
    "Epoch 1/20\n",
    "38s - loss: 11.0899 - acc: 0.3514 - val_loss: 11.1410 - val_acc: 0.3294\n",
    "Epoch 2/20\n",
    "32s - loss: 10.9127 - acc: 0.3562 - val_loss: 10.9368 - val_acc: 0.3515\n",
    "Epoch 3/20\n",
    "32s - loss: 10.7670 - acc: 0.3621 - val_loss: 10.9091 - val_acc: 0.3532\n",
    "Epoch 4/20\n",
    "32s - loss: 10.6393 - acc: 0.3684 - val_loss: 7.6373 - val_acc: 0.5500\n",
    "Epoch 5/20\n",
    "31s - loss: 3.6307 - acc: 0.7933 - val_loss: 1.4512 - val_acc: 0.9345\n",
    "Epoch 6/20\n",
    "31s - loss: 1.2234 - acc: 0.9497 - val_loss: 0.8745 - val_acc: 0.9730\n",
    "Epoch 7/20\n",
    "32s - loss: 0.9837 - acc: 0.9659 - val_loss: 0.7437 - val_acc: 0.9817\n",
    "Epoch 8/20\n",
    "32s - loss: 0.8285 - acc: 0.9757 - val_loss: 0.7250 - val_acc: 0.9811\n",
    "Epoch 9/20\n",
    "31s - loss: 0.7660 - acc: 0.9796 - val_loss: 0.6584 - val_acc: 0.9861\n",
    "Epoch 10/20\n",
    "31s - loss: 0.7029 - acc: 0.9832 - val_loss: 0.6142 - val_acc: 0.9891\n",
    "Epoch 11/20\n",
    "31s - loss: 0.6489 - acc: 0.9869 - val_loss: 0.5900 - val_acc: 0.9905\n",
    "Epoch 12/20\n",
    "32s - loss: 0.6206 - acc: 0.9883 - val_loss: 0.6310 - val_acc: 0.9871\n",
    "Epoch 13/20\n",
    "31s - loss: 0.5986 - acc: 0.9898 - val_loss: 0.5452 - val_acc: 0.9929\n",
    "Epoch 14/20\n",
    "31s - loss: 0.5825 - acc: 0.9902 - val_loss: 0.5657 - val_acc: 0.9909\n",
    "Epoch 15/20\n",
    "31s - loss: 0.5607 - acc: 0.9915 - val_loss: 0.5438 - val_acc: 0.9917\n",
    "Epoch 16/20\n",
    "31s - loss: 0.5384 - acc: 0.9926 - val_loss: 0.5082 - val_acc: 0.9941\n",
    "Epoch 17/20\n",
    "30s - loss: 0.5221 - acc: 0.9934 - val_loss: 0.4810 - val_acc: 0.9960\n",
    "Epoch 18/20\n",
    "30s - loss: 0.5113 - acc: 0.9937 - val_loss: 0.4792 - val_acc: 0.9955\n",
    "Epoch 19/20\n",
    "30s - loss: 0.4960 - acc: 0.9944 - val_loss: 0.4742 - val_acc: 0.9957\n",
    "Epoch 20/20\n",
    "30s - loss: 0.4788 - acc: 0.9951 - val_loss: 0.4865 - val_acc: 0.9946\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results:\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NNN=503\n",
    "pred_result = model.predict(train_score_images[NNN:NNN+1])\n",
    "pred_result = pred_result.reshape(1,64,64,2)\n",
    "pred_zero,pred_one = pred_result[:,:,:,0], pred_result[:,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(train_score_images[NNN].reshape(64,64),cmap=\"gray\")\n",
    "plt.imshow(pred_one.reshape(64,64),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*results were prettty good here, but didn't save those cells.*\n",
    "\n",
    "The edges were not straight so let's keep training... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rate = 1e-5\n",
    "num_epochs = 20\n",
    "model.optimizer=keras.optimizers.RMSprop(train_rate, decay=1-0.9995)\n",
    "model.fit_generator(train_generator, \n",
    "                    num_train_images//batch_size, num_epochs, \n",
    "                    verbose=2,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=num_valid_images//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NNN=505\n",
    "pred_result = model.predict(train_score_images[NNN:NNN+1])\n",
    "pred_result = pred_result.reshape(1,64,64,2)\n",
    "pred_zero,pred_one = pred_result[:,:,:,0], pred_result[:,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_score_images[NNN].reshape(64,64),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(pred_one.reshape(64,64),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice Results\n",
    "------------\n",
    "\n",
    "Save those weights, but this wasn't much effort so let's keep going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!mkdir data/results\n",
    "model.save_weights('data/results/mask_weights_171011_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rate = 1e-5\n",
    "num_epochs = 20\n",
    "model.optimizer=keras.optimizers.RMSprop(train_rate, decay=1-0.9995)\n",
    "model.fit_generator(train_generator, \n",
    "                    num_train_images//batch_size, num_epochs, \n",
    "                    verbose=2,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=num_valid_images//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('data/results/mask_weights_171011_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NNN=1060\n",
    "pred_result = model.predict(train_score_images[NNN:NNN+1])\n",
    "pred_result = pred_result.reshape(1,64,64,2)\n",
    "pred_zero,pred_one = pred_result[:,:,:,0], pred_result[:,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(pred_one.reshape(64,64),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use model\n",
    "------\n",
    "\n",
    "Read in full image, dice it up, predict output for each bit, reassemble into a full image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_names(base_name):\n",
    "    score_file_name = f'../data/{base_name}.png'\n",
    "    mask_file_name = f'../data/{base_name}_mask.png'\n",
    "    return score_file_name, mask_file_name\n",
    "\n",
    "def get_images(base_name):\n",
    "    score_file_name, mask_file_name = get_image_names(base_name)\n",
    "    mask_image = Image.open(mask_file_name).convert('L')\n",
    "    mask_image = Image.eval(mask_image, lambda x: x*10) # saturate the mask\n",
    "    score_image = Image.open(score_file_name)\n",
    "    background = Image.new('RGBA', score_image.size, (255,255,255))\n",
    "    score_image = Image.alpha_composite(background,score_image)\n",
    "    score_image = score_image.convert('L')\n",
    "    return score_image, mask_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_prediction_tiles(input_image):\n",
    "    # cannot just send an image to predict.  We have to send the tiles\n",
    "    # pred_result = model.predict(score_image)\n",
    "    #   ValueError: Error when checking : expected input_1 to have shape (None, 64, 64, 1) \n",
    "    #   but got array with shape (1, 1123, 794, 1)\n",
    "    num_tiles = np.ceil(np.array([input_image.width/64,input_image.height/64]))\n",
    "    tot_tiles = int(num_tiles[0]*num_tiles[1])\n",
    "    input_tiles = np.zeros((tot_tiles,64,64,1),dtype='uint8')\n",
    "    i = 0\n",
    "    for iy in range(int(num_tiles[1])):\n",
    "        for ix in range(int(num_tiles[0])):\n",
    "            input_tiles[i] = np.array(score_image.crop([ix*64,iy*64,(ix+1)*64,(iy+1)*64])).reshape(64,64,1)\n",
    "            i += 1\n",
    "    return input_tiles\n",
    "\n",
    "def image_from_tiles(width,height,pred_tiles,pred_channel):\n",
    "    # reshape & collect a single channel plane to visualize\n",
    "    pred_tiles = pred_tiles.reshape(input_tiles.shape[0],64,64,2)\n",
    "    pred_one_tiles = pred_tiles[:,:,:,1]#pred_one = pred_one.reshape(int(num_tiles[0])*64,int(num_tiles[1])*64)\n",
    "    num_tiles = np.ceil(np.array([width/64,height/64]))\n",
    "    pred_one = Image.new('L', (int(num_tiles[0])*64,int(num_tiles[1])*64), (0,))\n",
    "    i = 0\n",
    "    for iy in range(int(num_tiles[1])):\n",
    "        for ix in range(int(num_tiles[0])):\n",
    "            pred_tile_image = Image.fromarray(np.uint8(pred_one_tiles[i]*255))\n",
    "            pred_one.paste(pred_tile_image, (ix*64,iy*64))\n",
    "            i += 1\n",
    "    return pred_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_image, mask_image = get_images('gen_two')\n",
    "input_tiles = get_input_prediction_tiles(score_image)\n",
    "pred_tiles = model.predict(input_tiles)\n",
    "pred_one = image_from_tiles(score_image.width, score_image.height, pred_tiles, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the output\n",
    "----------\n",
    "\n",
    "Look's likely to be acceptable!  There are definitely some glitches, but I think these are \n",
    "results that would work for finding the regions to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for comparison, look at either of these...\n",
    "#score_image\n",
    "mask_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just some debug snippets\n",
    "ix, iy = 10, 0\n",
    "img = np.array(score_image.crop([ix*64,iy*64,(ix+1)*64,(iy+1)*64])).reshape(64,64,1)\n",
    "plt.imshow(img.reshape(64,64),cmap=\"gray\")\n",
    "#img[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(input_tiles[10].reshape(64,64),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
